use async_trait::async_trait;
use chrono::Utc;
use serde::{Deserialize, Serialize};
use tokio::task;

use crate::agents::{
    Agent, AgentBehavior, AgentCapability, AgentState, AgentType, CommunicationComplexity,
    Experience,
};
use crate::communication::patterns::{CommunicationConfig, MessagePriority};
use crate::communication::protocols::{MessageEnvelope, MessagePayload, MessageType};
use crate::neural::{CpuOptimizer, NLPProcessor, QuantizedOps, QuantizedWeights, VectorizedOps};
use crate::tasks::{Task, TaskResult};
use crate::utils::error::HiveResult;

/// CPU-optimized agent with SIMD acceleration and quantized neural processing
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizedAgent {
    pub base_agent: Agent,
    pub optimization_level: OptimizationLevel,
    pub quantized_capabilities: Option<QuantizedCapabilities>,
    pub performance_profile: PerformanceProfile,
    pub resource_constraints: ResourceConstraints,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum OptimizationLevel {
    Minimal,    // Basic optimizations, <512MB RAM
    Standard,   // Balanced optimizations, 512MB-2GB RAM
    Aggressive, // Full optimizations, >2GB RAM
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QuantizedCapabilities {
    pub weights: QuantizedWeights,
    pub capability_matrix: Vec<Vec<u8>>,
    pub scale_factors: Vec<f32>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceProfile {
    pub avg_task_time_ms: f64,
    pub memory_usage_mb: f64,
    pub cpu_utilization: f64,
    pub cache_hit_ratio: f64,
    pub simd_acceleration: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResourceConstraints {
    pub max_memory_mb: usize,
    pub max_cpu_threads: usize,
    pub battery_mode: bool,
    pub thermal_throttling: bool,
}

impl OptimizedAgent {
    #[must_use]
    pub fn new(name: String, agent_type: AgentType, optimization_level: OptimizationLevel) -> Self {
        let base_agent = Agent::new(name, agent_type);
        let resource_constraints = Self::detect_resource_constraints();

        Self {
            base_agent,
            optimization_level,
            quantized_capabilities: None,
            performance_profile: PerformanceProfile::default(),
            resource_constraints,
        }
    }

    /// Async version of new() that properly handles file I/O in async contexts
    pub async fn new_async(
        name: String,
        agent_type: AgentType,
        optimization_level: OptimizationLevel,
    ) -> Self {
        let base_agent = Agent::new(name, agent_type);
        let resource_constraints = Self::detect_resource_constraints_async().await;

        Self {
            base_agent,
            optimization_level,
            quantized_capabilities: None,
            performance_profile: PerformanceProfile::default(),
            resource_constraints,
        }
    }

    #[must_use]
    pub fn new_for_edge_device(name: String, agent_type: AgentType) -> Self {
        let mut agent = Self::new(name, agent_type, OptimizationLevel::Minimal);
        agent.resource_constraints = ResourceConstraints {
            max_memory_mb: 256,
            max_cpu_threads: 1,
            battery_mode: true,
            thermal_throttling: true,
        };
        agent
    }

    #[must_use]
    pub fn new_for_raspberry_pi(name: String, agent_type: AgentType) -> Self {
        let mut agent = Self::new(name, agent_type, OptimizationLevel::Standard);
        agent.resource_constraints = ResourceConstraints {
            max_memory_mb: 512,
            max_cpu_threads: 4,
            battery_mode: false,
            thermal_throttling: true,
        };
        agent
    }

    #[must_use]
    pub fn new_for_server(name: String, agent_type: AgentType) -> Self {
        let mut agent = Self::new(name, agent_type, OptimizationLevel::Aggressive);
        agent.resource_constraints = ResourceConstraints {
            max_memory_mb: 4096,
            max_cpu_threads: 8,
            battery_mode: false,
            thermal_throttling: false,
        };
        agent
    }

    fn detect_resource_constraints() -> ResourceConstraints {
        let available_memory = Self::get_available_memory_mb();
        let cpu_count = num_cpus::get();

        ResourceConstraints {
            max_memory_mb: (available_memory * 0.8) as usize, // Use 80% of available memory
            max_cpu_threads: cpu_count.min(4),                // Limit to 4 threads for efficiency
            battery_mode: Self::is_battery_powered(),
            thermal_throttling: false,
        }
    }

    async fn detect_resource_constraints_async() -> ResourceConstraints {
        // Use spawn_blocking for file I/O operations to avoid blocking the async runtime
        let (available_memory, battery_mode) =
            task::spawn_blocking(|| (Self::get_available_memory_mb(), Self::is_battery_powered()))
                .await
                .unwrap_or_else(|_| {
                    tracing::warn!(
                        "Failed to detect system resources asynchronously, using defaults"
                    );
                    (2048.0, false)
                });

        let cpu_count = num_cpus::get();

        ResourceConstraints {
            max_memory_mb: (available_memory * 0.8) as usize, // Use 80% of available memory
            max_cpu_threads: cpu_count.min(4),                // Limit to 4 threads for efficiency
            battery_mode,
            thermal_throttling: false,
        }
    }

    fn get_available_memory_mb() -> f64 {
        // Optimized memory detection with better error handling
        #[cfg(target_os = "linux")]
        {
            // Use std::fs::read_to_string for synchronous contexts
            // In async contexts, this should be called via spawn_blocking
            if let Ok(meminfo) = std::fs::read_to_string("/proc/meminfo") {
                // More efficient parsing - look for MemAvailable first
                for line in meminfo.lines() {
                    if line.starts_with("MemAvailable:") {
                        if let Some(kb_str) = line.split_whitespace().nth(1) {
                            if let Ok(kb) = kb_str.parse::<u64>() {
                                return kb as f64 / 1024.0; // Convert KB to MB
                            }
                        }
                    }
                }
            } else {
                tracing::debug!("Failed to read /proc/meminfo, using fallback memory detection");
            }
        }

        // Fallback: assume 2GB available
        2048.0
    }

    fn is_battery_powered() -> bool {
        // Simplified battery detection
        #[cfg(target_os = "linux")]
        {
            std::path::Path::new("/sys/class/power_supply/BAT0").exists()
        }
        #[cfg(not(target_os = "linux"))]
        {
            false
        }
    }

    /// Optimize agent capabilities using quantization
    #[allow(clippy::unnecessary_wraps)]
    pub fn optimize_capabilities(&mut self) -> HiveResult<()> {
        let capability_count = self.base_agent.capabilities.len();
        if capability_count == 0 {
            return Ok(());
        }

        // Create capability matrix for quantization
        let matrix_size = capability_count * 10; // 10 features per capability
        let mut capability_matrix: Vec<f32> = Vec::with_capacity(matrix_size);

        for capability in &self.base_agent.capabilities {
            // Convert capability to feature vector
            let features = self.capability_to_features(capability);
            capability_matrix.extend(features);
        }

        // Quantize the capability matrix based on optimization level
        match self.optimization_level {
            OptimizationLevel::Minimal => {
                // Use 8-bit quantization for maximum memory savings
                let quantized_weights = QuantizedOps::quantize_weights(&capability_matrix);
                self.quantized_capabilities = Some(QuantizedCapabilities {
                    weights: quantized_weights,
                    capability_matrix: vec![vec![0u8; 10]; capability_count],
                    scale_factors: vec![1.0; capability_count],
                });
            }
            OptimizationLevel::Standard => {
                // Use 16-bit quantization for balanced performance
                let _quantized_weights_16 =
                    QuantizedOps::quantize_weights_16bit(&capability_matrix);
                // Convert to 8-bit format for compatibility
                let quantized_weights = QuantizedOps::quantize_weights(&capability_matrix);
                self.quantized_capabilities = Some(QuantizedCapabilities {
                    weights: quantized_weights,
                    capability_matrix: vec![vec![0u8; 10]; capability_count],
                    scale_factors: vec![1.0; capability_count],
                });
            }
            OptimizationLevel::Aggressive => {
                // Keep full precision but optimize memory layout
                let quantized_weights = QuantizedOps::quantize_weights(&capability_matrix);
                self.quantized_capabilities = Some(QuantizedCapabilities {
                    weights: quantized_weights,
                    capability_matrix: vec![vec![0u8; 10]; capability_count],
                    scale_factors: vec![1.0; capability_count],
                });
            }
        }

        tracing::info!(
            "ðŸš€ Optimized {} capabilities with {:?} level",
            capability_count,
            self.optimization_level
        );
        Ok(())
    }

    #[allow(clippy::unused_self)]
    fn capability_to_features(&self, capability: &AgentCapability) -> Vec<f32> {
        // Convert capability to 10-dimensional feature vector
        vec![
            capability.proficiency as f32,
            capability.learning_rate as f32,
            capability.name.len() as f32 / 20.0, // Normalized name length
            1.0,                                 // Bias term
            (capability.proficiency * capability.learning_rate) as f32, // Interaction term
            capability.proficiency.powi(2) as f32, // Squared proficiency
            capability.learning_rate.powi(2) as f32, // Squared learning rate
            f64::midpoint(capability.proficiency, capability.learning_rate) as f32, // Average
            (capability.proficiency - capability.learning_rate) as f32, // Difference
            0.5,                                 // Reserved for future features
        ]
    }

    /// Calculate task fitness using optimized operations
    #[must_use]
    pub fn calculate_optimized_task_fitness(&self, task: &Task) -> f64 {
        let required_caps = &task.required_capabilities;
        let mut fitness_scores = Vec::new();
        let mut weights = Vec::new();

        for req_cap in required_caps {
            if let Some(agent_cap) = self
                .base_agent
                .capabilities
                .iter()
                .find(|c| c.name == req_cap.name)
            {
                fitness_scores.push(agent_cap.proficiency as f32);
                weights.push(1.0); // Default weight since field doesn't exist
            }
        }

        if !fitness_scores.is_empty() {
            // Use vectorized weighted average calculation
            let weighted_sum = VectorizedOps::dot_product(&fitness_scores, &weights);
            let weight_sum: f32 = weights.iter().sum();

            if weight_sum > 0.0 {
                return f64::from(weighted_sum / weight_sum);
            }
        }

        0.5 // Default fitness
    }

    /// Update performance profile with new measurements
    pub fn update_performance_profile(&mut self, task_time_ms: f64, memory_usage_mb: f64) {
        let alpha = 0.1; // Exponential moving average factor

        self.performance_profile.avg_task_time_ms =
            alpha * task_time_ms + (1.0 - alpha) * self.performance_profile.avg_task_time_ms;

        self.performance_profile.memory_usage_mb =
            alpha * memory_usage_mb + (1.0 - alpha) * self.performance_profile.memory_usage_mb;

        // Update CPU utilization (simplified)
        self.performance_profile.cpu_utilization = self.measure_cpu_utilization();

        // Check if we're using SIMD acceleration
        let optimizer = CpuOptimizer::new();
        self.performance_profile.simd_acceleration =
            optimizer.simd_support.avx2 || optimizer.simd_support.neon;
    }

    fn measure_cpu_utilization(&self) -> f64 {
        // Simplified CPU utilization measurement
        // In real implementation, use system APIs
        match self.optimization_level {
            OptimizationLevel::Minimal => 0.3,
            OptimizationLevel::Standard => 0.5,
            OptimizationLevel::Aggressive => 0.7,
        }
    }

    /// Adaptive resource management
    pub fn adapt_to_resources(&mut self) {
        let current_memory = self.performance_profile.memory_usage_mb;
        let max_memory = self.resource_constraints.max_memory_mb as f64;

        if current_memory > max_memory * 0.9 {
            // Memory pressure - reduce optimization level
            self.optimization_level = match self.optimization_level {
                OptimizationLevel::Aggressive => OptimizationLevel::Standard,
                OptimizationLevel::Standard | OptimizationLevel::Minimal => {
                    OptimizationLevel::Minimal
                }
            };

            tracing::warn!(
                "ðŸ”¥ Memory pressure detected, reducing optimization to {:?}",
                self.optimization_level
            );
        } else if current_memory < max_memory * 0.5 {
            // Memory available - increase optimization level
            self.optimization_level = match self.optimization_level {
                OptimizationLevel::Minimal => OptimizationLevel::Standard,
                OptimizationLevel::Standard | OptimizationLevel::Aggressive => {
                    OptimizationLevel::Aggressive
                }
            };
        }
    }

    /// Get optimization statistics
    #[must_use]
    pub fn get_optimization_stats(&self) -> OptimizationStats {
        let memory_savings = match self.optimization_level {
            OptimizationLevel::Minimal => 0.7,    // 70% memory savings
            OptimizationLevel::Standard => 0.5,   // 50% memory savings
            OptimizationLevel::Aggressive => 0.3, // 30% memory savings
        };

        let speed_improvement = if self.performance_profile.simd_acceleration {
            match self.optimization_level {
                OptimizationLevel::Minimal => 2.0,    // 2x speedup
                OptimizationLevel::Standard => 4.0,   // 4x speedup
                OptimizationLevel::Aggressive => 8.0, // 8x speedup
            }
        } else {
            1.0 // No SIMD acceleration
        };

        OptimizationStats {
            optimization_level: self.optimization_level.clone(),
            memory_savings_percent: memory_savings * 100.0,
            speed_improvement_factor: speed_improvement,
            simd_enabled: self.performance_profile.simd_acceleration,
            quantization_enabled: self.quantized_capabilities.is_some(),
            current_memory_mb: self.performance_profile.memory_usage_mb,
            avg_task_time_ms: self.performance_profile.avg_task_time_ms,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OptimizationStats {
    pub optimization_level: OptimizationLevel,
    pub memory_savings_percent: f64,
    pub speed_improvement_factor: f64,
    pub simd_enabled: bool,
    pub quantization_enabled: bool,
    pub current_memory_mb: f64,
    pub avg_task_time_ms: f64,
}

impl Default for PerformanceProfile {
    fn default() -> Self {
        Self {
            avg_task_time_ms: 100.0,
            memory_usage_mb: 10.0,
            cpu_utilization: 0.5,
            cache_hit_ratio: 0.8,
            simd_acceleration: false,
        }
    }
}

#[async_trait]
impl AgentBehavior for OptimizedAgent {
    async fn execute_task(&mut self, task: Task) -> HiveResult<TaskResult> {
        // Standardized state management
        let _previous_state = self.base_agent.state;
        self.base_agent.state = AgentState::Working;
        self.base_agent.last_active = Utc::now();

        let start_time = std::time::Instant::now();
        let start_memory = self.performance_profile.memory_usage_mb;

        // Use optimized fitness calculation
        let fitness = self.calculate_optimized_task_fitness(&task);
        let success_probability = fitness * 0.8 + 0.2;

        // Simulate task execution with optimization-aware processing
        let processing_time = match self.optimization_level {
            OptimizationLevel::Minimal => 200,   // Slower but memory efficient
            OptimizationLevel::Standard => 100,  // Balanced
            OptimizationLevel::Aggressive => 50, // Fastest
        };

        tokio::time::sleep(tokio::time::Duration::from_millis(processing_time)).await;

        let success = rand::random::<f64>() < success_probability;
        let execution_time = start_time.elapsed().as_millis() as u64;

        // Create experience with optimization metadata
        let experience = Experience {
            timestamp: Utc::now(),
            task_type: task.task_type.clone(),
            success,
            context: format!(
                "{} (optimized: {:?})",
                task.description, self.optimization_level
            ),
            learned_insight: if success {
                Some(format!(
                    "Successfully completed {} task with {:?} optimization",
                    task.task_type, self.optimization_level
                ))
            } else {
                Some(format!(
                    "Failed {} task - optimization level: {:?}",
                    task.task_type, self.optimization_level
                ))
            },
        };

        self.base_agent.learn_from_experience(experience);

        // Update performance metrics
        let elapsed_ms = execution_time as f64;
        let current_memory = start_memory + 5.0; // Simulate memory usage
        self.update_performance_profile(elapsed_ms, current_memory);

        // Adaptive resource management
        self.adapt_to_resources();

        // Standardized state restoration
        self.base_agent.state = AgentState::Idle;

        Ok(TaskResult {
            task_id: task.id,
            agent_id: self.base_agent.id,
            success,
            output: if success {
                format!(
                    "Task completed successfully by optimized agent {} ({}x speedup)",
                    self.base_agent.name,
                    self.get_optimization_stats().speed_improvement_factor
                )
            } else {
                format!(
                    "Task failed - optimized agent {} needs more training",
                    self.base_agent.name
                )
            },
            error_message: if success {
                None
            } else {
                Some("Optimization failed".to_string())
            },
            completed_at: Utc::now(),
            execution_time,
            quality_score: if success { Some(0.8) } else { Some(0.2) },
            learned_insights: vec!["CPU optimization applied".to_string()],
        })
    }

    async fn communicate(
        &mut self,
        envelope: MessageEnvelope,
    ) -> HiveResult<Option<MessageEnvelope>> {
        // Standardized state management
        let _previous_state = self.base_agent.state;
        self.base_agent.state = AgentState::Communicating;
        self.base_agent.last_active = Utc::now();

        // Use optimized text processing for communication
        let start_time = std::time::Instant::now();

        // Use optimized communication processing with standardized delays
        let complexity = match envelope.priority {
            MessagePriority::Low => CommunicationComplexity::Simple,
            MessagePriority::Normal => CommunicationComplexity::Standard,
            MessagePriority::High => CommunicationComplexity::Complex,
            MessagePriority::Critical => CommunicationComplexity::Heavy,
        };

        // Use standardized delay (optimized agents get faster processing)
        let base_delay = match (complexity, &self.optimization_level) {
            (CommunicationComplexity::Simple, OptimizationLevel::Aggressive) => 10,
            (CommunicationComplexity::Simple, _) => 25,
            (CommunicationComplexity::Standard, OptimizationLevel::Aggressive) => 25,
            (CommunicationComplexity::Standard, _) => 50,
            (CommunicationComplexity::Complex, OptimizationLevel::Aggressive) => 50,
            (CommunicationComplexity::Complex, _) => 100,
            (CommunicationComplexity::Heavy, OptimizationLevel::Aggressive) => 100,
            (CommunicationComplexity::Heavy, _) => 200,
        };

        tokio::time::sleep(tokio::time::Duration::from_millis(base_delay)).await;

        // Process the message based on type with optimization-specific handling
        let response = match envelope.message_type {
            MessageType::Request => {
                let response_payload = match &envelope.payload {
                    MessagePayload::Text(text) => {
                        MessagePayload::Text(format!(
                            "Optimized agent {} responding to {}: Acknowledged - {} (processed in {}ms, SIMD: {})",
                            self.base_agent.name,
                            envelope.sender_id,
                            text,
                            start_time.elapsed().as_millis(),
                            self.performance_profile.simd_acceleration
                        ))
                    }
                    MessagePayload::Json(json) => {
                        MessagePayload::Json(serde_json::json!({
                            "response": format!("Optimized agent {} acknowledged request", self.base_agent.name),
                            "processing_time_ms": start_time.elapsed().as_millis(),
                            "optimization_level": format!("{:?}", self.optimization_level),
                            "simd_enabled": self.performance_profile.simd_acceleration,
                            "original_request": json
                        }))
                    }
                    _ => MessagePayload::Text(format!(
                        "Optimized agent {} acknowledged message (processed in {}ms)",
                        self.base_agent.name,
                        start_time.elapsed().as_millis()
                    )),
                };

                Some(MessageEnvelope::new_response(
                    &envelope,
                    self.base_agent.id,
                    response_payload,
                ))
            }
            MessageType::Broadcast => {
                // For broadcasts, log with optimization metrics
                tracing::info!(
                    "Optimized agent {} received broadcast from {}: {:?} (optimization: {:?}, SIMD: {})",
                    self.base_agent.name,
                    envelope.sender_id,
                    envelope.payload,
                    self.optimization_level,
                    self.performance_profile.simd_acceleration
                );
                None
            }
            MessageType::TaskAssigned => {
                // Handle task assignment with optimization info
                if let MessagePayload::TaskInfo { task_id, .. } = &envelope.payload {
                    tracing::info!(
                        "Optimized agent {} received task assignment: {} (level: {:?})",
                        self.base_agent.name,
                        task_id,
                        self.optimization_level
                    );
                }
                None
            }
            _ => {
                let response = MessageEnvelope::new_response(
                    &envelope,
                    self.base_agent.id,
                    MessagePayload::Text(format!(
                        "Optimized agent {} processed message of type {:?} (processed in {}ms, optimization: {:?})",
                        self.base_agent.name,
                        envelope.message_type,
                        start_time.elapsed().as_millis(),
                        self.optimization_level
                    )),
                );
                Some(response)
            }
        };

        // Update performance metrics
        let elapsed_ms = start_time.elapsed().as_millis() as f64;
        self.update_performance_profile(elapsed_ms, self.performance_profile.memory_usage_mb);

        // Standardized state restoration
        self.base_agent.state = AgentState::Idle;
        Ok(response)
    }

    async fn request_response(
        &mut self,
        request: MessageEnvelope,
        timeout: std::time::Duration,
    ) -> HiveResult<MessageEnvelope> {
        // Use optimized request-response with faster processing
        // Optimized agents process requests faster
        let processing_time = match self.optimization_level {
            OptimizationLevel::Minimal => timeout / 4,
            OptimizationLevel::Standard => timeout / 6,
            OptimizationLevel::Aggressive => timeout / 8,
        };

        tokio::time::sleep(processing_time).await;

        let response = MessageEnvelope::new_response(
            &request,
            self.base_agent.id,
            MessagePayload::Text(format!(
                "Optimized agent {} processed request with timeout {:?} (optimization: {:?})",
                self.base_agent.name, timeout, self.optimization_level
            )),
        );

        Ok(response)
    }

    fn get_communication_config(&self) -> CommunicationConfig {
        // Optimized agents have different communication configurations
        let mut config = CommunicationConfig::default();

        match self.optimization_level {
            OptimizationLevel::Minimal => {
                config.max_retries = 2;
                config.default_timeout = std::time::Duration::from_secs(15);
            }
            OptimizationLevel::Standard => {
                config.max_retries = 3;
                config.default_timeout = std::time::Duration::from_secs(10);
                config.enable_compression = true;
            }
            OptimizationLevel::Aggressive => {
                config.max_retries = 3;
                config.default_timeout = std::time::Duration::from_secs(5);
                config.enable_compression = true;
                config.max_concurrent_messages = 2000;
            }
        }

        config
    }

    async fn learn(&mut self, nlp_processor: &NLPProcessor) -> HiveResult<()> {
        // Standardized state management is handled by base agent
        let start_time = std::time::Instant::now();

        let result = self.base_agent.learn(nlp_processor).await;

        let elapsed_ms = start_time.elapsed().as_millis() as f64;
        self.update_performance_profile(elapsed_ms, self.performance_profile.memory_usage_mb + 2.0);

        result
    }

    async fn update_position(
        &mut self,
        swarm_center: (f64, f64),
        neighbors: &[Agent],
    ) -> HiveResult<()> {
        // Use optimized position calculations
        let start_time = std::time::Instant::now();

        // Convert neighbor positions to f32 for vectorized operations
        let neighbor_positions: Vec<(f32, f32)> = neighbors
            .iter()
            .map(|n| (n.position.0 as f32, n.position.1 as f32))
            .collect();

        // Optimized swarm calculations using vectorized operations
        let mut separation = (0.0f32, 0.0f32);
        let mut alignment = (0.0f32, 0.0f32);
        let mut cohesion = (0.0f32, 0.0f32);

        let current_pos = (
            self.base_agent.position.0 as f32,
            self.base_agent.position.1 as f32,
        );
        let mut neighbor_count = 0;

        // Vectorized neighbor processing
        for &neighbor_pos in &neighbor_positions {
            let dx = neighbor_pos.0 - current_pos.0;
            let dy = neighbor_pos.1 - current_pos.1;
            let distance = (dx * dx + dy * dy).sqrt();

            if distance < 50.0 && distance > 0.1 {
                neighbor_count += 1;

                // Separation
                if distance < 20.0 {
                    separation.0 += current_pos.0 - neighbor_pos.0;
                    separation.1 += current_pos.1 - neighbor_pos.1;
                }

                // Alignment and cohesion
                alignment.0 += neighbor_pos.0;
                alignment.1 += neighbor_pos.1;
                cohesion.0 += neighbor_pos.0;
                cohesion.1 += neighbor_pos.1;
            }
        }

        if neighbor_count > 0 {
            let inv_count = 1.0 / neighbor_count as f32;
            alignment.0 *= inv_count;
            alignment.1 *= inv_count;
            cohesion.0 *= inv_count;
            cohesion.1 *= inv_count;
        }

        // Apply forces with optimization-aware weights
        let force_multiplier = match self.optimization_level {
            OptimizationLevel::Minimal => 0.05,    // Conservative movement
            OptimizationLevel::Standard => 0.1,    // Standard movement
            OptimizationLevel::Aggressive => 0.15, // Aggressive movement
        };

        let new_x = current_pos.0
            + separation.0 * force_multiplier
            + alignment.0 * force_multiplier * 0.5
            + cohesion.0 * force_multiplier * 0.5
            + (swarm_center.0 as f32 - current_pos.0) * force_multiplier * 0.1;

        let new_y = current_pos.1
            + separation.1 * force_multiplier
            + alignment.1 * force_multiplier * 0.5
            + cohesion.1 * force_multiplier * 0.5
            + (swarm_center.1 as f32 - current_pos.1) * force_multiplier * 0.1;

        self.base_agent.position = (f64::from(new_x), f64::from(new_y));

        // Update performance metrics
        let elapsed_ms = start_time.elapsed().as_millis() as f64;
        self.update_performance_profile(elapsed_ms, self.performance_profile.memory_usage_mb);

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::agents::AgentType;

    #[test]
    fn test_optimized_agent_creation() {
        let agent = OptimizedAgent::new(
            "TestAgent".to_string(),
            AgentType::Worker,
            OptimizationLevel::Standard,
        );

        assert_eq!(agent.base_agent.name, "TestAgent");
        assert!(matches!(
            agent.optimization_level,
            OptimizationLevel::Standard
        ));
    }

    #[test]
    fn test_edge_device_configuration() {
        let agent = OptimizedAgent::new_for_edge_device("EdgeAgent".to_string(), AgentType::Worker);

        assert!(agent.resource_constraints.max_memory_mb <= 256);
        assert_eq!(agent.resource_constraints.max_cpu_threads, 1);
        assert!(agent.resource_constraints.battery_mode);
    }

    #[test]
    fn test_capability_optimization() {
        let mut agent = OptimizedAgent::new(
            "TestAgent".to_string(),
            AgentType::Worker,
            OptimizationLevel::Minimal,
        );

        agent.base_agent.add_capability(AgentCapability {
            name: "test_capability".to_string(),
            proficiency: 0.8,
            learning_rate: 0.1,
        });

        let result = agent.optimize_capabilities();
        assert!(result.is_ok());
        assert!(agent.quantized_capabilities.is_some());
    }
}
