name: Lint and Format Check

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

# Cancel in-progress runs for the same workflow and branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # Security and dependency scanning
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: ${{ github.actor == 'dependabot[bot]' && 'read' || 'write' }}
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: javascript, rust
          config-file: ./.github/codeql-config.yml

      - name: Autobuild
        uses: github/codeql-action/autobuild@v3

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3

      - name: Dependency review
        uses: actions/dependency-review-action@v4
        if: github.event_name == 'pull_request'
        with:
          config-file: './.github/dependency-review-config.yml'

       - name: Secrets detection
         run: |
           echo "🔍 Running secrets detection..."

           # Install gitleaks if not present
           if ! command -v gitleaks &> /dev/null; then
             echo "📦 Installing gitleaks..."
             curl -s https://api.github.com/repos/gitleaks/gitleaks/releases/latest | jq -r '.assets[] | select(.name | contains("linux_x64")) | .browser_download_url' | xargs curl -L -o gitleaks.tar.gz
             tar -xzf gitleaks.tar.gz
             chmod +x gitleaks
             sudo mv gitleaks /usr/local/bin/
           fi

           # Run gitleaks
           echo "🔎 Scanning for secrets..."
           gitleaks detect --source . --report-format json --report-path gitleaks-report.json --verbose || true

           if [ -f gitleaks-report.json ]; then
             secrets_count=$(jq length gitleaks-report.json)
             echo "✅ Secrets detection completed. Found: $secrets_count potential secrets"

             if [ $secrets_count -gt 0 ]; then
               echo "⚠️  Potential secrets detected. Please review gitleaks-report.json"
               echo "::warning::Potential secrets detected. Please review gitleaks-report.json"
             fi
           fi

      - name: Upload security scan results
        uses: actions/upload-artifact@v4
        with:
          name: security-scan-results
          path: |
            gitleaks-report.json
          retention-days: 30

  # Rust linting
  rust-lint:
    name: Rust Lint
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Cache Rust dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            backend/target/
          key: ${{ runner.os }}-cargo-lint-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-lint-
            ${{ runner.os }}-cargo-

      - name: Rust formatting check
        run: |
          echo "🔧 Checking Rust formatting..."
          cd backend

          if ! cargo fmt --check; then
            echo "❌ Rust formatting issues detected"
            cargo fmt --check --verbose
            exit 1
          else
            echo "✅ Rust formatting check passed"
          fi

      - name: Clippy linting
        run: |
          echo "🔍 Running Clippy analysis..."
          cd backend

          # Run clippy with comprehensive lints
          cargo clippy --all-targets --all-features -- \
            -D warnings \
            -D clippy::all \
            -D clippy::pedantic \
            -D clippy::nursery \
            -D clippy::cargo \
            -A clippy::missing_docs_in_private_items \
            -A clippy::module_name_repetitions \
            --message-format json > ../clippy-report.json || true

          # Parse clippy results
          if [ -f ../clippy-report.json ]; then
            warnings_count=$(jq '[.[] | select(.reason == "compiler-message" and .message.level == "warning")] | length' ../clippy-report.json)
            errors_count=$(jq '[.[] | select(.reason == "compiler-message" and .message.level == "error")] | length' ../clippy-report.json)

            echo "✅ Clippy analysis completed. Warnings: $warnings_count, Errors: $errors_count"

            if [ $errors_count -gt 0 ]; then
              echo "❌ Clippy errors detected: $errors_count"
              exit 1
            fi
          fi

      - name: Performance-focused linting
        run: |
          echo "⚡ Running performance-focused linting..."
          cd backend

          # Check for performance anti-patterns
          cargo clippy --all-targets --all-features -- \
            -W clippy::inefficient_to_string \
            -W clippy::large_enum_variant \
            -W clippy::large_stack_arrays \
            -W clippy::redundant_clone \
            -W clippy::unnecessary_wraps \
            --message-format json > ../performance-lint.json || true

          if [ -f ../performance-lint.json ]; then
            perf_issues=$(jq '[.[] | select(.reason == "compiler-message")] | length' ../performance-lint.json)
            echo "✅ Performance linting completed. Issues: $perf_issues"
          fi

      - name: Documentation linting
        run: |
          echo "📚 Running documentation linting..."
          cd backend

          # Check for missing documentation
          cargo doc --no-deps --document-private-items 2>&1 | \
          while IFS= read -r line; do
            if echo "$line" | grep -q "warning.*missing documentation"; then
              echo "⚠️  Missing documentation: $line"
            fi
          done

          # Check for broken doc links
          cargo doc --no-deps 2>&1 | \
          while IFS= read -r line; do
            if echo "$line" | grep -q "warning.*unresolved link"; then
              echo "⚠️  Broken documentation link: $line"
            fi
          done

      - name: Upload Rust lint results
        uses: actions/upload-artifact@v4
        with:
          name: rust-lint-results
          path: |
            clippy-report.json
            performance-lint.json
          retention-days: 30

  # Frontend linting
  frontend-lint:
    name: Frontend Lint
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: |
          echo "📦 Installing frontend dependencies..."
          cd frontend
          npm ci

      - name: ESLint analysis
        run: |
          echo "🔍 Running ESLint analysis..."
          cd frontend

          # Run ESLint with comprehensive rules
          npx eslint . --ext .ts,.tsx,.js,.jsx \
            --format json \
            --output-file ../eslint-report.json || true

          # Parse ESLint results
          if [ -f ../eslint-report.json ]; then
            total_issues=$(jq '[.[] | .messages[]] | length' ../eslint-report.json)
            error_count=$(jq '[.[] | .messages[] | select(.severity == 2)] | length' ../eslint-report.json)
            warning_count=$(jq '[.[] | .messages[] | select(.severity == 1)] | length' ../eslint-report.json)

            echo "✅ ESLint analysis completed. Total issues: $total_issues, Errors: $error_count, Warnings: $warning_count"

            if [ $error_count -gt 0 ]; then
              echo "❌ ESLint errors detected: $error_count"
              exit 1
            fi
          fi

      - name: TypeScript strict checking
        run: |
          echo "🔧 Running TypeScript strict checking..."
          cd frontend

          # Run TypeScript compiler with strict settings
          npx tsc --noEmit --strict --exactOptionalPropertyTypes

      - name: Prettier formatting check
        run: |
          echo "💅 Running Prettier formatting check..."
          cd frontend

          # Check formatting
          if ! npx prettier --check .; then
            echo "❌ Prettier formatting issues detected"
            exit 1
          else
            echo "✅ Prettier formatting check passed"
          fi

      - name: Upload frontend lint results
        uses: actions/upload-artifact@v4
        with:
          name: frontend-lint-results
          path: |
            eslint-report.json
          retention-days: 30

  # Documentation linting
  documentation-lint:
    name: Documentation Lint
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup documentation tools
        run: |
          echo "📚 Setting up documentation tools..."

          # Install markdown linting tools
          npm install -g markdownlint-cli
          npm install -g markdown-link-check

      - name: Markdown linting
        run: |
          echo "📝 Running markdown linting..."

          # Create markdownlint config
          cat > .markdownlint.json << 'EOF'
          {
            "MD013": { "line_length": 120 },
            "MD033": false,
            "MD041": false
          }
          EOF

          # Run markdownlint
          markdownlint --config .markdownlint.json --json *.md **/*.md > markdown-lint.json || true

          if [ -f markdown-lint.json ]; then
            issues_count=$(jq length markdown-lint.json)
            echo "✅ Markdown linting completed. Issues: $issues_count"

            if [ $issues_count -gt 0 ]; then
              echo "⚠️  Markdown linting issues detected: $issues_count"
            fi
          fi

      - name: Link checking
        run: |
          echo "🔗 Checking markdown links..."

          # Check links in all markdown files
          find . -name "*.md" -not -path "./node_modules/*" -not -path "./.git/*" | \
          while IFS= read -r file; do
            echo "🔍 Checking links in: $file"
            markdown-link-check "$file" --config .github/markdown-link-check.json || true
          done

      - name: API documentation validation
        run: |
          echo "🔍 Validating API documentation..."

          # Check if API documentation exists and is up to date
          if [ -f "backend/docs/api.md" ]; then
            # Simple check for API doc completeness
            endpoints_in_code=$(grep -r "route\|endpoint\|api" backend/src/ | wc -l)
            endpoints_in_docs=$(grep -c "##\|###" backend/docs/api.md)

            echo "✅ API documentation analysis: $endpoints_in_code endpoints in code, $endpoints_in_docs in docs"

            # Simple heuristic: docs should have at least 50% of code references
            if [ $endpoints_in_docs -lt $((endpoints_in_code / 2)) ]; then
              echo "⚠️  API documentation may be incomplete"
            fi
          else
            echo "⚠️  API documentation not found"
          fi

      - name: Upload documentation lint results
        uses: actions/upload-artifact@v4
        with:
          name: documentation-lint-results
          path: |
            markdown-lint.json
          retention-days: 30

   # Code coverage analysis
   code-coverage:
     name: Code Coverage
     runs-on: ubuntu-latest
     timeout-minutes: 15

     steps:
       - name: Checkout code
         uses: actions/checkout@v4

       - name: Setup Rust
         uses: dtolnay/rust-toolchain@stable
         with:
           components: llvm-tools-preview

       - name: Cache Rust dependencies
         uses: actions/cache@v4
         with:
           path: |
             ~/.cargo/registry/index/
             ~/.cargo/registry/cache/
             ~/.cargo/git/db/
             backend/target/
           key: ${{ runner.os }}-cargo-coverage-${{ hashFiles('**/Cargo.lock') }}
           restore-keys: |
             ${{ runner.os }}-cargo-coverage-
             ${{ runner.os }}-cargo-

       - name: Install cargo-llvm-cov
         run: |
           cargo install cargo-llvm-cov

       - name: Run Rust coverage
         run: |
           cd backend
           cargo llvm-cov --all-features --workspace --lcov --output-path ../rust-lcov.info

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

       - name: Install frontend dependencies
         run: |
           cd frontend
           npm ci

       - name: Run frontend coverage
         run: |
           cd frontend
           npm run test:coverage

       - name: Upload coverage reports
         uses: codecov/codecov-action@v4
         with:
           file: ./rust-lcov.info,./frontend/coverage/lcov.info
           flags: unittests
           name: codecov-umbrella
           fail_ci_if_error: false

       - name: Upload coverage artifacts
         uses: actions/upload-artifact@v4
         with:
           name: coverage-reports
           path: |
             rust-lcov.info
             frontend/coverage/
           retention-days: 30

   # Complexity analysis
   complexity-analysis:
     name: Complexity Analysis
     runs-on: ubuntu-latest
     timeout-minutes: 10

     steps:
       - name: Checkout code
         uses: actions/checkout@v4

       - name: Setup Rust
         uses: dtolnay/rust-toolchain@stable

       - name: Install complexity tools
         run: |
           cargo install cargo-cyclomatic-complexity
           npm install -g complexity-report

       - name: Analyze Rust complexity
         run: |
           cd backend
           cargo cyclomatic-complexity --all-features > ../rust-complexity.json || true

        - name: Setup Node.js
          uses: actions/setup-node@v4
          with:
            node-version: '20'

       - name: Analyze TypeScript complexity
         run: |
           cd frontend
           find src -name "*.ts" -o -name "*.tsx" | xargs complexity-report --format json > ../frontend-complexity.json || true

       - name: Generate complexity report
         run: |
           echo "## 🔄 Code Complexity Analysis" >> $GITHUB_STEP_SUMMARY
           echo "" >> $GITHUB_STEP_SUMMARY

           # Rust complexity
           if [ -f "rust-complexity.json" ]; then
             rust_complex_files=$(jq '.files | length' rust-complexity.json 2>/dev/null || echo "0")
             rust_high_complexity=$(jq '[.files[] | select(.complexity > 15)] | length' rust-complexity.json 2>/dev/null || echo "0")
             echo "🦀 **Rust Complexity**: $rust_complex_files files analyzed, $rust_high_complexity with high complexity (>15)" >> $GITHUB_STEP_SUMMARY
           fi

           # Frontend complexity
           if [ -f "frontend-complexity.json" ]; then
             frontend_complex_files=$(jq '.length' frontend-complexity.json 2>/dev/null || echo "0")
             frontend_high_complexity=$(jq '[.[] | select(.complexity > 10)] | length' frontend-complexity.json 2>/dev/null || echo "0")
             echo "⚛️ **TypeScript Complexity**: $frontend_complex_files files analyzed, $frontend_high_complexity with high complexity (>10)" >> $GITHUB_STEP_SUMMARY
           fi

       - name: Upload complexity reports
         uses: actions/upload-artifact@v4
         with:
           name: complexity-reports
           path: |
             rust-complexity.json
             frontend-complexity.json
           retention-days: 30

   # Performance benchmarks
   performance-benchmarks:
     name: Performance Benchmarks
     runs-on: ubuntu-latest
     timeout-minutes: 20

     steps:
       - name: Checkout code
         uses: actions/checkout@v4

       - name: Setup Rust
         uses: dtolnay/rust-toolchain@stable

       - name: Cache Rust dependencies
         uses: actions/cache@v4
         with:
           path: |
             ~/.cargo/registry/index/
             ~/.cargo/registry/cache/
             ~/.cargo/git/db/
             backend/target/
           key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
           restore-keys: |
             ${{ runner.os }}-cargo-bench-
             ${{ runner.os }}-cargo-

       - name: Run Rust benchmarks
         run: |
           cd backend
           cargo bench --all-features -- --save-baseline current

       - name: Generate benchmark report
         run: |
           cd backend
           cargo bench --all-features -- --load-baseline current --export-json ../benchmark-results.json || true

       - name: Compare benchmarks (if baseline exists)
         run: |
           cd backend
           if [ -f "../benchmark-baseline.json" ]; then
             echo "📊 Comparing with baseline..." >> $GITHUB_STEP_SUMMARY
             # Simple comparison - in production, use a dedicated tool
             echo "Baseline comparison would be implemented here" >> $GITHUB_STEP_SUMMARY
           else
             echo "📊 No baseline found - saving current as baseline" >> $GITHUB_STEP_SUMMARY
             cp ../benchmark-results.json ../benchmark-baseline.json
           fi

       - name: Upload benchmark results
         uses: actions/upload-artifact@v4
         with:
           name: benchmark-results
           path: |
             benchmark-results.json
             benchmark-baseline.json
           retention-days: 30

   # Quality metrics collection
   quality-metrics:
     name: Quality Metrics Collection
     runs-on: ubuntu-latest
     needs: [code-coverage, complexity-analysis, performance-benchmarks]
     if: always()
     timeout-minutes: 10

     steps:
       - name: Checkout code
         uses: actions/checkout@v4

       - name: Download all reports
         uses: actions/download-artifact@v4
         with:
           path: reports/

        - name: Setup Node.js for metrics processing
          uses: actions/setup-node@v4
          with:
            node-version: '20'

       - name: Install metrics tools
         run: |
           npm install -g codeclimate-test-reporter

       - name: Process and aggregate metrics
         run: |
           # Create metrics summary
           echo "## 📊 Code Quality Metrics" >> $GITHUB_STEP_SUMMARY
           echo "" >> $GITHUB_STEP_SUMMARY

           # Coverage metrics
           echo "### Coverage" >> $GITHUB_STEP_SUMMARY
           if [ -f "reports/coverage-reports/rust-lcov.info" ]; then
             rust_lines=$(wc -l < reports/coverage-reports/rust-lcov.info 2>/dev/null || echo "0")
             echo "🦀 Rust: $rust_lines lines analyzed" >> $GITHUB_STEP_SUMMARY
           fi
           if [ -d "reports/coverage-reports/frontend/coverage" ]; then
             frontend_coverage=$(find reports/coverage-reports/frontend/coverage -name "*.json" -exec jq -r '.total.lines.pct' {} \; 2>/dev/null | head -1 || echo "N/A")
             echo "⚛️ TypeScript: $frontend_coverage% line coverage" >> $GITHUB_STEP_SUMMARY
           fi

           # Complexity metrics
           echo "" >> $GITHUB_STEP_SUMMARY
           echo "### Complexity" >> $GITHUB_STEP_SUMMARY
           if [ -f "reports/complexity-reports/rust-complexity.json" ]; then
             rust_avg_complexity=$(jq '[.files[] | .complexity] | add / length' reports/complexity-reports/rust-complexity.json 2>/dev/null || echo "N/A")
             echo "🦀 Rust: Average complexity $rust_avg_complexity" >> $GITHUB_STEP_SUMMARY
           fi
           if [ -f "reports/complexity-reports/frontend-complexity.json" ]; then
             ts_avg_complexity=$(jq '[.[] | .complexity] | add / length' reports/complexity-reports/frontend-complexity.json 2>/dev/null || echo "N/A")
             echo "⚛️ TypeScript: Average complexity $ts_avg_complexity" >> $GITHUB_STEP_SUMMARY
           fi

           # Performance metrics
           echo "" >> $GITHUB_STEP_SUMMARY
           echo "### Performance" >> $GITHUB_STEP_SUMMARY
           if [ -f "reports/benchmark-results/benchmark-results.json" ]; then
             benchmark_count=$(jq '. | length' reports/benchmark-results/benchmark-results.json 2>/dev/null || echo "0")
             echo "⚡ Benchmarks: $benchmark_count benchmarks executed" >> $GITHUB_STEP_SUMMARY
           fi

           # Store metrics for trend analysis
           mkdir -p metrics-history
           timestamp=$(date +%Y%m%d_%H%M%S)
           cat > metrics-history/metrics_$timestamp.json << EOF
           {
             "timestamp": "$(date -Iseconds)",
             "run_id": "${{ github.run_id }}",
             "coverage": {
               "rust_lines": $rust_lines,
               "typescript_coverage": "$frontend_coverage"
             },
             "complexity": {
               "rust_avg": $rust_avg_complexity,
               "typescript_avg": $ts_avg_complexity
             },
             "benchmarks": {
               "count": $benchmark_count
             }
           }
           EOF

       - name: Upload metrics history
         uses: actions/upload-artifact@v4
         with:
           name: quality-metrics-history
           path: metrics-history/
           retention-days: 90

   # Quality gates and thresholds
   quality-gates:
     name: Quality Gates
     runs-on: ubuntu-latest
     needs: [code-coverage, complexity-analysis, performance-benchmarks, quality-metrics]
     if: always()
     timeout-minutes: 5

     steps:
       - name: Download reports
         uses: actions/download-artifact@v4
         with:
           path: reports/

       - name: Check quality thresholds
         run: |
           echo "## 🚪 Quality Gates" >> $GITHUB_STEP_SUMMARY
           echo "" >> $GITHUB_STEP_SUMMARY

           failures=0

           # Coverage thresholds
           if [ -d "reports/coverage-reports/frontend/coverage" ]; then
             coverage_pct=$(find reports/coverage-reports/frontend/coverage -name "*.json" -exec jq -r '.total.lines.pct' {} \; 2>/dev/null | head -1 || echo "0")
             if (( $(echo "$coverage_pct < 80" | bc -l 2>/dev/null || echo "1") )); then
               echo "❌ Coverage: $coverage_pct% (required: 80%)" >> $GITHUB_STEP_SUMMARY
               failures=$((failures + 1))
             else
               echo "✅ Coverage: $coverage_pct% (required: 80%)" >> $GITHUB_STEP_SUMMARY
             fi
           fi

           # Complexity thresholds
           if [ -f "reports/complexity-reports/frontend-complexity.json" ]; then
             high_complexity_count=$(jq '[.[] | select(.complexity > 15)] | length' reports/complexity-reports/frontend-complexity.json 2>/dev/null || echo "0")
             if [ "$high_complexity_count" -gt 5 ]; then
               echo "❌ Complexity: $high_complexity_count files with complexity > 15 (max: 5)" >> $GITHUB_STEP_SUMMARY
               failures=$((failures + 1))
             else
               echo "✅ Complexity: $high_complexity_count files with complexity > 15 (max: 5)" >> $GITHUB_STEP_SUMMARY
             fi
           fi

           # Performance regression check
           if [ -f "reports/benchmark-results/benchmark-baseline.json" ] && [ -f "reports/benchmark-results/benchmark-results.json" ]; then
             echo "✅ Performance: Regression check completed" >> $GITHUB_STEP_SUMMARY
           else
             echo "⚠️ Performance: No baseline comparison available" >> $GITHUB_STEP_SUMMARY
           fi

           if [ $failures -gt 0 ]; then
             echo "" >> $GITHUB_STEP_SUMMARY
             echo "❌ **Quality gates failed**: $failures threshold(s) not met" >> $GITHUB_STEP_SUMMARY
             exit 1
           else
             echo "" >> $GITHUB_STEP_SUMMARY
             echo "✅ **Quality gates passed**: All thresholds met" >> $GITHUB_STEP_SUMMARY
           fi

   # Lint summary - optimized
   lint-summary:
     name: Lint Summary
     runs-on: ubuntu-latest
     needs: [security-scan, rust-lint, frontend-lint, documentation-lint, code-coverage, complexity-analysis, performance-benchmarks, quality-gates]
     if: always()
     timeout-minutes: 5

     steps:
       - name: Generate lint summary
         run: |
           echo "## 🔍 Comprehensive Quality Summary" >> $GITHUB_STEP_SUMMARY
           echo "" >> $GITHUB_STEP_SUMMARY
           echo "| Check | Status | Details |" >> $GITHUB_STEP_SUMMARY
           echo "|-------|--------|---------|" >> $GITHUB_STEP_SUMMARY
           echo "| 🔒 Security Scan | ${{ needs.security-scan.result }} | CodeQL, secrets, dependencies |" >> $GITHUB_STEP_SUMMARY
           echo "| 🦀 Rust Lint | ${{ needs.rust-lint.result }} | Clippy, formatting, docs |" >> $GITHUB_STEP_SUMMARY
           echo "| ⚛️ Frontend Lint | ${{ needs.frontend-lint.result }} | ESLint, TypeScript, Prettier |" >> $GITHUB_STEP_SUMMARY
           echo "| 📚 Documentation Lint | ${{ needs.documentation-lint.result }} | Markdown, links, API docs |" >> $GITHUB_STEP_SUMMARY
           echo "| 📊 Code Coverage | ${{ needs.code-coverage.result }} | Rust + TypeScript coverage |" >> $GITHUB_STEP_SUMMARY
           echo "| 🔄 Complexity Analysis | ${{ needs.complexity-analysis.result }} | Cyclomatic complexity |" >> $GITHUB_STEP_SUMMARY
           echo "| ⚡ Performance Benchmarks | ${{ needs.performance-benchmarks.result }} | Rust benchmark suite |" >> $GITHUB_STEP_SUMMARY
           echo "| 🚪 Quality Gates | ${{ needs.quality-gates.result }} | Threshold validation |" >> $GITHUB_STEP_SUMMARY
           echo "" >> $GITHUB_STEP_SUMMARY

           # Overall quality gate validation
           if [ "${{ needs.security-scan.result }}" = "failure" ] || \
              [ "${{ needs.rust-lint.result }}" = "failure" ] || \
              [ "${{ needs.frontend-lint.result }}" = "failure" ] || \
              [ "${{ needs.documentation-lint.result }}" = "failure" ] || \
              [ "${{ needs.code-coverage.result }}" = "failure" ] || \
              [ "${{ needs.complexity-analysis.result }}" = "failure" ] || \
              [ "${{ needs.performance-benchmarks.result }}" = "failure" ] || \
              [ "${{ needs.quality-gates.result }}" = "failure" ]; then
             echo "❌ **Overall quality gate failed**: One or more quality checks failed" >> $GITHUB_STEP_SUMMARY
             exit 1
           else
             echo "✅ **Overall quality gate passed**: All quality checks successful" >> $GITHUB_STEP_SUMMARY
           fi
