{
  "architecture": {
    "type": "transformer",
    "layers": 12,
    "attention_heads": 8,
    "hidden_size": 768,
    "feedforward_size": 3072
  },
  "training": {
    "optimizer": "adam",
    "learning_rate": 0.001,
    "batch_size": 32,
    "epochs": 100,
    "loss_function": "cross_entropy"
  },
  "regularization": {
    "dropout": 0.1,
    "weight_decay": 0.01,
    "gradient_clipping": 1.0
  }
}
